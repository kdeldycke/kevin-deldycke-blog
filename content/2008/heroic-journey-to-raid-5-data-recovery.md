---
date: "2008-07-24"
title: "Heroic journey to RAID-5 data recovery"
category: English
tags: array, Backup, disk, drive, Hardware, HDD, Linux, mdadm, monitoring, RAID, Server, system, UPS
---

Last week there was a power grid failure which break down my server's RAID array. I have no [UPS](https://en.wikipedia.org/wiki/Uninterruptible_power_supply) (as I'm a skinflint) and no automatic email alerts (because I'm too lazy to set it up). As a result, for 5 days, my 3-disk [RAID-5 array](https://en.wikipedia.org/wiki/RAID_5) was relying on only 2 disks until I noticed the issue...

By using a combination of following commands, I was soon aware of the gravity of the situation:

    ```shell-session
    $ cat /proc/mdstat
    $ mdadm --examine /dev/sda1
    ```

My `/dev/sda1` disk was kicked out of the array, so I did the right stuff which consisted of reconstructing the array:

    ```shell-session
    $ mdadm /dev/md0 -a /dev/sda1
    ```

Then, in an unlucky combination of cosmic ray bombardment, spooky action at a distance and astrological misalignment, half-way to the end of the rebuilding process (which can take up to 5 hours), another disk failed! It was late, I was tired and utterly worried about losing 1.5 To of precious data. In such a bad shape, I was afraid to worsen the situation. So I decided to shutdown the server and sleep on the problem.

The next day I tried to boot my server to find it (surprise!) stuck in the middle of the boot process, with the famous message:

    ```text
    hit control-D to continue or give root password to fix manually
    ```

This is "normal" as my server tried to mount the [ext3 filesystem](https://en.wikipedia.org/wiki/Ext3) from the `/dev/md0` partition that was just assembled by `mdadm`. Of course `md0`, if assembled and available to the system, was not running because only one disk, out of three, was in a clean state.

I skip here the epic substory in which I wasted days in a search of a working keyboard, but I let you imagine how such adventures makes my week...

Eventually, I was able to analyze the situation in details. My first reflex? Check that disks are not physically dead:

    ```shell-session
    $ fdisk -l /dev/sda
    $ fdisk -l /dev/sdb
    $ fdisk -l /dev/sdc
    ```

"Linux raid partitions" (type code "`fd`") are still there. Good. I assumed here that disks where not physically damaged. Maybe I should have looked at [S.M.A.R.T.](https://en.wikipedia.org/wiki/Self-Monitoring,_Analysis,_and_Reporting_Technology) datas and statistics (via [smartmontools](https://smartmontools.sourceforge.net)). But remember, I'm lazy (and a bit crazy).

The next step was to get informations about the RAID array itself using:

    ```shell-session
    $ mdadm --detail /dev/md0
    ```

which output the status table below (probably inaccurate as I reconstructed it afterwards):

    ```text
    Number   Major   Minor   RaidDevice State
       0       0        0        0      removed
       1       0        0        1      faulty removed
       2       8       33        2      active sync   /dev/sdc1
       3       8       17        3      spare
    ```

What this table told us?

  * The array is up, but not running. One of its device (`sdc1`) was clean and active, but it's not enough to get a working RAID-5.

  * My first attempt to rebuild the array lead to an unexpected result: it added `sda1` as a spare device (in slot #3).

  * It confirm that `sdb1` unexpectedly failed and is now in a bad state ("`faulty removed`").

Then I stopped the array and tried to fearlessly (re)assemble it using 3 differents methods:

    ```shell-session
    $ mdadm -S /dev/md0
    $ mdadm -A /dev/md0
    $ mdadm --assemble /dev/md0 --verbose /dev/sd[abc]1
    $ mdadm --assemble --force --scan /dev/md0 --verbose
    ```

It always failed with messages like:

    ```text
    mdadm: failed to RUN_ARRAY /dev/md0: Input/output error
    mdadm: /dev/md0 assembled from 1 drives and 1 spare - not enough to start the array.
    ```

So I examined each drive from `mdadm`'s point of view:

    ```shell-session
    $ mdadm -E /dev/sda1
    $ mdadm -E /dev/sdb1
    $ mdadm -E /dev/sdc1
    $ mdadm -E /dev/sd[abc]1 | grep Event
    ```

The lastest command compare the "`Event`" attribute of all devices. It output something like:

    ```text
    Events : 0.53120
    Events : 0.53108
    Events : 0.53120
    ```

which indicate that `sda1` and `sdc1` are somewhat synced (share the same number) and `sdb1` "late" (lower number).

Here I've got the idea of recreating the raid array without `sdb1`, relying only on `sda1` and `sdc1`, by using the "magic" (hence dangerous) `--assume-clean` option. The latter doesn't build, erase or initialize a new array. It just try to assemble it "as is". Here is the command:

    ```shell-session
    $ mdadm --create /dev/md0 --assume-clean --level=5 --verbose --raid-devices=3 /dev/sda1 missing /dev/sdc1
    ```

And it worked! :D

I mounted the `md0` partition and cleaned it up:

    ```shell-session
    $ fsck.ext3 -v /dev/md0
    $ mount /dev/md0
    ```

I updated my [mdadm](https://neil.brown.name/blog/mdadm) configuration before rebooting my server:

    ```shell-session
    $ mdadm --detail --scan >> /etc/mdadm/mdadm.conf
    $ vi /etc/mdadm/mdadm.conf
    $ reboot
    ```

But history repeat itself, and again, the system hang up during boot. Except this time I knew what was happening: the boot process detected the remaining `sdb1` device as part of the old array (the one before the regeneration I did above) and tried to run it. [Remembering my last year post]({filename}/2007/how-to-recover-a-raid-array-after-having-zero-ized-superblocks.md), I zero-ized the superblock of `sdb1`:

    ```shell-session
    $ mdadm -S /dev/md0
    $ mdadm --zero-superblock /dev/sdb1
    ```

A server reboot proved I was right and my `md0` partition was automagically mounted in altered state:

    ```shell-session
    $ cat /proc/mdstat
    Personalities : [raid6] [raid5] [raid4]
    md0 : active raid5 sdb1[3] sda1[0] sdc1[2]
          1465143808 blocks level 5, 64k chunk, algorithm 2 [3/2] [U_U]

    unused devices: <none>
    ```

I just had to re-add `sdb1` to fill the available slot and update the mdadm configuration to get back my array in its initial state:

    ```shell-session
    $ mdadm --manage /dev/md0 --add /dev/sdb1
    $ mdadm --detail --scan >> /etc/mdadm/mdadm.conf
    $ vi /etc/mdadm/mdadm.conf
    ```
